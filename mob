#!/usr/bin/env python
# coding=utf8

import argparse
from urlparse import urlparse

from ministryofbackup import *
from ministryofbackup.backend import FilesystemBackend

def backend_url(v):
    o = urlparse(v)

    if not o.scheme in ('file',):
        raise ValueError('Unknown scheme: %s' % o.scheme)

    return o


parser = argparse.ArgumentParser()
parser.set_defaults(loglevel=logbook.NOTICE)
parser.add_argument('directory')
parser.add_argument('destination', type=backend_url)
parser.add_argument('--db', default='fingerprints.db')
parser.add_argument('-d', '--debug', action='store_true', default=False)

logargs = parser.add_mutually_exclusive_group()
logargs.add_argument('-v', '--verbose', const=logbook.INFO,
                     action='store_const', dest='loglevel')
logargs.add_argument('-q', '--quiet', const=logbook.WARNING,
                     action='store_const', dest='loglevel')

args = parser.parse_args()

logbook.NullHandler().push_application()
logbook.StderrHandler(
    level=logbook.DEBUG if args.debug else args.loglevel
).push_application()

base = os.path.abspath(args.directory)
log.debug("Base directory: %s" % base)

if os.path.exists(args.db):
    log.notice("Loading fingerprint database '%s'" % args.db)
    with open(args.db, 'rb') as f:
        db = Database.load(base, f)
else:
    log.notice("New fingerprint database")
    db = Database(base)

if args.debug:
    log.debug("META, CONTENT, RELNAME")
    for rel_name, meta_print in db.meta_prints.iteritems():
        log.debug('%s %s %s' % (hexlify(meta_print),\
              hexlify(db.content_prints[rel_name]),\
              rel_name))

# collect filenames on filesystem
db.load_meta()

log.notice("Collected %d files in %d directories" % (len(db.files),
                                                   len(db.dirs)))

new, updated = db.get_new_and_updated_files()

# altered file checks with progress-bar
altered = []
if updated:  # could also force checking all files here with cmdline arg?
    pbar = progressbar.ProgressBar(widgets=DATA_PROGRESS_BAR,
                                   maxval=db.get_sizes_of(updated))
    pbar.start()
    altered = db.get_altered_files(updated, progress=pbar.update)
    pbar.finish()
deleted = db.get_deleted_files()

log.notice("Found %d new files, %d updated, %d altered and %d deleted files"\
         % (len(new), len(updated), len(altered), len(deleted)))

if args.loglevel >= logbook.INFO:
    for rel_name in new:
        log.info("N %s" % rel_name)
    for rel_name in updated:
        log.info("U %s" % rel_name)
    for rel_name in altered:
        log.info("A %s" % rel_name)
    for rel_name in deleted:
        log.info("D %s" % rel_name)

# create pipes
tarpipe_r, tarpipe_w = os.pipe()
compress_r, compress_w = os.pipe()

backend = FilesystemBackend(args.destination.path)
storagefd = backend.open_backup_archive('somebackup')

def _target_compress(*args, **kwargs):
    os.close(tarpipe_w)
    multitar.compress(*args, **kwargs)

def _target_encrypt(*args, **kwargs):
    os.close(tarpipe_w)
    os.close(compress_w)
    multitar.encrypt(*args, **kwargs)

comp_p = multiprocessing.Process(
    target=_target_compress,
    kwargs={
        'srcfd': tarpipe_r,
        'destfd': compress_w
    }
)

enc_p = multiprocessing.Process(
    target=_target_encrypt,
    kwargs={
        'srcfd': compress_r,
        'destfd': storagefd,
        'password': 'foo'
    },
)

comp_p.daemon = True
comp_p.start()
enc_p.daemon = True
enc_p.start()

os.close(compress_w)

# start compression process
with os.fdopen(tarpipe_w, 'wb') as tar_w,\
tarfile.open(mode='w|', fileobj=tar_w) as archive:
    for rel_name in chain(new, altered):
        fm = db.files[rel_name]
        log.debug('adding %r to archive' % fm.path)
        tarinfo = archive.gettarinfo(fm.path, rel_name)
        r = fm.open_read()
        archive.addfile(tarinfo, r)

        # tarinfo reads stats bytes, trigger end-of-file detection
        assert '' == r.read()

log.debug('Waiting for compression process to finish...')
comp_p.join()
enc_p.join()

os.close(storagefd)


# FIXME: upload file data here

# create index file?

# transition over
log.notice("Updating database")
db.update_meta()

# FIXME: upload database to s3?
log.debug("Writing to database")

with open(args.db, 'wb') as f:
    db.dump(f)
